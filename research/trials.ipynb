{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install pinecone-client\n",
    "# !pip install pinecone\n",
    "# !pip install sentence_transformers\n",
    "# !pip install huggingface_hub==0.25.2\n",
    "# !pip install \"pinecone[grpc]\"\n",
    "\n",
    "#!pip3 install pinecone==5.0.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\mchatbot\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.vectorstores import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PINECONE_API_KEY = \"pcsk_4hZwV6_Qk4yj6FHJGPEqkii7FQFeZ3junJuKnx4Qmnxw4MCk8GQTUh7HM19knvJe41hVef\"\n",
    "#PINECONE_API_ENV = \"us-east1-aws\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Pinecone client with your API key\n",
    "pc = Pinecone(api_key=\"pcsk_4hZwV6_Qk4yj6FHJGPEqkii7FQFeZ3junJuKnx4Qmnxw4MCk8GQTUh7HM19knvJe41hVef\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Create an index with the correct dimension (384)\n",
    "pinecone.create_index(index_name, dimension=384, metric=\"cosine\")\n",
    "\n",
    "# Connect to the index\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# Use HuggingFaceEmbeddings to create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Creating Embeddings for each of the text chunks and storing\n",
    "text_chunks = [\"your text chunk 1\", \"your text chunk 2\", ...]  # Replace with your actual text chunks\n",
    "docsearch = Pinecone.from_texts(\n",
    "    texts=[t for t in text_chunks],\n",
    "    embeddings=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query_text = \"your query text\"\n",
    "query_embeddings = embeddings.encode([query_text])\n",
    "result = index.query(queries=query_embeddings, top_k=5)\n",
    "print(result)\n",
    "\n",
    "# Optional: Delete the index\n",
    "pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the pdf\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents = loader.load()\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of my chunk: 20990\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()\n",
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello World\")\n",
    "print(\"length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vdo\n",
    "#Initializing the Pinecone\n",
    "#pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)\n",
    "pinecone.init(api_key=PINECONE_API_KEY)\n",
    "\n",
    "\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "#Creating Embeddings for Each of the Text Chunks and storing\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in text_chunks], embeddings, index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we already have an index we can load it like this\n",
    "docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
    "\n",
    "query = \"What are Allergies\"\n",
    "\n",
    "docs = docsearch.similarity_search(query, k=3)\n",
    "\n",
    "print(\"Result\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                    model_type=\"llama\",\n",
    "                    config={'max_new_tokens':512,\n",
    "                            'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input=input(f\"Input Prompt:\")\n",
    "    result=qa({\"query\": user_input})\n",
    "    print(\"Response : \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below code is just observe not need to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "pc = Pinecone(api_key=\"pcsk_6K37fx_4ojcVFjUNuemDqFEp8bs3oGYu2gX8XeL6ACbjGqtFwciSWqrFu7hXaFsiHqWqQD\")\n",
    "index = pc.Index(\"medical-chatbot\")\n",
    "\n",
    "#Creating Embeddings for Each of the Text Chunks and storing\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in text_chunks], embeddings, index_name=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "from google.api import annotations_pb2\n",
    "from google.protobuf import json_format\n",
    "\n",
    "print(\"Modules imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# # Set your Pinecone API key\n",
    "PINECONE_API_KEY = \"pcsk_6K37fx_4ojcVFjUNuemDqFEp8bs3oGYu2gX8XeL6ACbjGqtFwciSWqrFu7hXaFsiHqWqQD\"\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Define your index name\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "# Connect to the existing index\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# Use HuggingFaceEmbeddings to create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creating Embeddings for each of the text chunks and storing\n",
    "#text_chunks = [\"your text chunk 1\", \"your text chunk 2\", ...]  # Replace with your actual text chunks\n",
    "docsearch = Pinecone.from_texts(\n",
    "    texts=[t for t in text_chunks],\n",
    "    embeddings=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query_text = \"your query text\"\n",
    "query_embeddings = embeddings.encode([query_text])\n",
    "result = index.query(queries=query_embeddings, top_k=5)\n",
    "print(result)\n",
    "\n",
    "# Optional: Delete the index\n",
    "# pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless index\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "pc.create_index(\n",
    "    name=\"example-index1\",\n",
    "    dimension=1536,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set your Pinecone API key\n",
    "PINECONE_API_KEY = \"pcsk_6K37fx_4ojcVFjUNuemDqFEp8bs3oGYu2gX8XeL6ACbjGqtFwciSWqrFu7hXaFsiHqWqQD\"\n",
    "\n",
    "# Initialize Pinecone Client\n",
    "client = pinecone.Client(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Define your index name\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "# Check if the index already exists\n",
    "if index_name not in client.list_indexes():\n",
    "    client.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        pod_type=\"p1\",  # Specify pod type for serverless index\n",
    "        replicas=1,     # Number of replicas\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    "\n",
    "# Connect to the existing index\n",
    "index = client.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Function to create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    return text_splitter.split_documents(extracted_data)\n",
    "\n",
    "# Function to download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Use the provided functions\n",
    "extracted_data = [{\"page_content\": \"Your extracted text content 1.\"}, {\"page_content\": \"Your extracted text content 2.\"}]\n",
    "text_chunks = text_split(extracted_data)\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "\n",
    "# Creating Embeddings for each of the text chunks and storing them in Pinecone\n",
    "docsearch = Pinecone.from_texts(\n",
    "    texts=[t[\"page_content\"] for t in text_chunks],\n",
    "    embeddings=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query_text = \"your query text\"\n",
    "query_embeddings = embeddings.encode([query_text])\n",
    "result = index.query(queries=query_embeddings, top_k=5)\n",
    "print(result)\n",
    "\n",
    "# Optional: Delete the index\n",
    "# client.delete_index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set your Pinecone API key\n",
    "PINECONE_API_KEY = \"pcsk_6K37fx_4ojcVFjUNuemDqFEp8bs3oGYu2gX8XeL6ACbjGqtFwciSWqrFu7hXaFsiHqWqQD\"\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=\"us-west1-gcp\")\n",
    "\n",
    "# Define your index name\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "# Check if the index already exists\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        pod_type=\"p1\"\n",
    "    )\n",
    "\n",
    "# Connect to the existing index\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# Function to create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "# Function to download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Use the provided functions\n",
    "extracted_data = [{\"page_content\": \"Your extracted text content 1.\"}, {\"page_content\": \"Your extracted text content 2.\"}]\n",
    "text_chunks = text_split(extracted_data)\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "\n",
    "# Creating Embeddings for each of the text chunks and storing them in Pinecone\n",
    "docsearch = Pinecone.from_texts(\n",
    "    texts=[t[\"page_content\"] for t in text_chunks],\n",
    "    embeddings=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query_text = \"your query text\"\n",
    "query_embeddings = embeddings.encode([query_text])\n",
    "result = index.query(queries=query_embeddings, top_k=5)\n",
    "print(result)\n",
    "\n",
    "# Optional: Delete the index\n",
    "# pinecone.delete_index(index_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
